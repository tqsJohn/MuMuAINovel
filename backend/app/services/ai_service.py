"""AIæœåŠ¡å°è£… - ç»Ÿä¸€çš„OpenAIå’ŒClaudeæ¥å£"""
from typing import Optional, AsyncGenerator, List, Dict, Any
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from app.config import settings as app_settings
from app.logger import get_logger
import httpx
import json

logger = get_logger(__name__)


class AIService:
    """AIæœåŠ¡ç»Ÿä¸€æ¥å£ - æ”¯æŒä»ç”¨æˆ·è®¾ç½®æˆ–å…¨å±€é…ç½®åˆå§‹åŒ–"""
    
    def __init__(
        self,
        api_provider: Optional[str] = None,
        api_key: Optional[str] = None,
        api_base_url: Optional[str] = None,
        default_model: Optional[str] = None,
        default_temperature: Optional[float] = None,
        default_max_tokens: Optional[int] = None
    ):
        """
        åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼ˆä¼˜åŒ–å¹¶å‘æ€§èƒ½ï¼‰
        
        Args:
            api_provider: APIæä¾›å•† (openai/anthropic)ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            api_key: APIå¯†é’¥ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            api_base_url: APIåŸºç¡€URLï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            default_model: é»˜è®¤æ¨¡å‹ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            default_temperature: é»˜è®¤æ¸©åº¦ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            default_max_tokens: é»˜è®¤æœ€å¤§tokensï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
        """
        # ä¿å­˜ç”¨æˆ·è®¾ç½®æˆ–ä½¿ç”¨å…¨å±€é…ç½®
        self.api_provider = api_provider or app_settings.default_ai_provider
        self.default_model = default_model or app_settings.default_model
        self.default_temperature = default_temperature or app_settings.default_temperature
        self.default_max_tokens = default_max_tokens or app_settings.default_max_tokens
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        openai_key = api_key if api_provider == "openai" else app_settings.openai_api_key
        if openai_key:
            try:
                limits = httpx.Limits(
                    max_keepalive_connections=50,
                    max_connections=100,
                    keepalive_expiry=30.0
                )
                
                http_client = httpx.AsyncClient(
                    timeout=httpx.Timeout(connect=60.0, read=180.0, write=60.0, pool=60.0),
                    limits=limits,
                    headers={
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    }
                )
                
                client_kwargs = {
                    "api_key": openai_key,
                    "http_client": http_client
                }
                
                base_url = api_base_url if api_provider == "openai" else app_settings.openai_base_url
                if base_url:
                    client_kwargs["base_url"] = base_url
                
                self.openai_client = AsyncOpenAI(**client_kwargs)
                self.openai_http_client = http_client
                self.openai_api_key = openai_key
                self.openai_base_url = base_url
                logger.info("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.openai_client = None
                self.openai_http_client = None
                self.openai_api_key = None
                self.openai_base_url = None
        else:
            self.openai_client = None
            self.openai_http_client = None
            self.openai_api_key = None
            self.openai_base_url = None
            # åªæœ‰å½“ç”¨æˆ·æ˜ç¡®é€‰æ‹©OpenAIä½œä¸ºæä¾›å•†æ—¶æ‰è­¦å‘Š
            if self.api_provider == "openai":
                logger.warning("âš ï¸ OpenAI API keyæœªé…ç½®ï¼Œä½†è¢«è®¾ç½®ä¸ºå½“å‰AIæä¾›å•†")
        
        # åˆå§‹åŒ–Anthropicå®¢æˆ·ç«¯
        anthropic_key = api_key if api_provider == "anthropic" else app_settings.anthropic_api_key
        if anthropic_key:
            try:
                limits = httpx.Limits(
                    max_keepalive_connections=50,
                    max_connections=100,
                    keepalive_expiry=30.0
                )
                
                http_client = httpx.AsyncClient(
                    timeout=httpx.Timeout(connect=60.0, read=180.0, write=60.0, pool=60.0),
                    limits=limits,
                    headers={
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    }
                )
                
                client_kwargs = {
                    "api_key": anthropic_key,
                    "http_client": http_client
                }
                
                base_url = api_base_url if api_provider == "anthropic" else app_settings.anthropic_base_url
                if base_url:
                    client_kwargs["base_url"] = base_url
                
                self.anthropic_client = AsyncAnthropic(**client_kwargs)
                logger.info("âœ… Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.anthropic_client = None
        else:
            self.anthropic_client = None
            # åªæœ‰å½“ç”¨æˆ·æ˜ç¡®é€‰æ‹©Anthropicä½œä¸ºæä¾›å•†æ—¶æ‰è­¦å‘Š
            if self.api_provider == "anthropic":
                logger.warning("âš ï¸ Anthropic API keyæœªé…ç½®ï¼Œä½†è¢«è®¾ç½®ä¸ºå½“å‰AIæä¾›å•†")
    
    async def generate_text(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            provider: AIæä¾›å•† (openai/anthropic)
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆMCPå·¥å…·æ ¼å¼ï¼‰
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥ (auto/required/none)
            
        Returns:
            DictåŒ…å«:
            - content: æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼‰
            - tool_calls: å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆå¦‚æœAIå†³å®šè°ƒç”¨å·¥å…·ï¼‰
            - finish_reason: å®ŒæˆåŸå› 
        """
        provider = provider or self.api_provider
        model = model or self.default_model
        temperature = temperature or self.default_temperature
        max_tokens = max_tokens or self.default_max_tokens
        
        if provider == "openai":
            return await self._generate_openai_with_tools(
                prompt, model, temperature, max_tokens, system_prompt, tools, tool_choice
            )
        elif provider == "anthropic":
            return await self._generate_anthropic_with_tools(
                prompt, model, temperature, max_tokens, system_prompt, tools, tool_choice
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")
    
    async def generate_text_stream(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            provider: AIæä¾›å•†
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        provider = provider or self.api_provider
        model = model or self.default_model
        temperature = temperature or self.default_temperature
        max_tokens = max_tokens or self.default_max_tokens
        
        if provider == "openai":
            async for chunk in self._generate_openai_stream(
                prompt, model, temperature, max_tokens, system_prompt
            ):
                yield chunk
        elif provider == "anthropic":
            async for chunk in self._generate_anthropic_stream(
                prompt, model, temperature, max_tokens, system_prompt
            ):
                yield chunk
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")
    
    async def _generate_openai(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> str:
        """ä½¿ç”¨OpenAIç”Ÿæˆæ–‡æœ¬"""
        if not self.openai_http_client:
            raise ValueError("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨OpenAI APIï¼ˆç›´æ¥HTTPè¯·æ±‚ï¼‰")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - æ¸©åº¦: {temperature}")
            logger.info(f"  - æœ€å¤§tokens: {max_tokens}")
            logger.info(f"  - Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            url = f"{self.openai_base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openai_api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            logger.debug(f"  - è¯·æ±‚URL: {url}")
            logger.debug(f"  - è¯·æ±‚å¤´: Authorization=Bearer ***")
            
            response = await self.openai_http_client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            
            data = response.json()
            
            logger.info(f"âœ… OpenAI APIè°ƒç”¨æˆåŠŸ")
            logger.info(f"  - å“åº”ID: {data.get('id', 'N/A')}")
            logger.info(f"  - é€‰é¡¹æ•°é‡: {len(data.get('choices', []))}")
            logger.debug(f"  - å®Œæ•´APIå“åº”: {data}")
            
            if not data.get('choices'):
                logger.error("âŒ OpenAIè¿”å›çš„choicesä¸ºç©º")
                raise ValueError("APIè¿”å›çš„å“åº”æ ¼å¼é”™è¯¯ï¼šchoiceså­—æ®µä¸ºç©º")
            
            choice = data['choices'][0]
            message = choice.get('message', {})
            finish_reason = choice.get('finish_reason')
            
            # DeepSeek R1ç‰¹æ®Šå¤„ç†ï¼šåªä½¿ç”¨contentï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œå¿½ç•¥reasoning_contentï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
            # reasoning_contentæ˜¯AIçš„æ€è€ƒè¿‡ç¨‹ï¼Œä¸æ˜¯æˆ‘ä»¬éœ€è¦çš„JSONç»“æœ
            content = message.get('content', '')
            
            # æ£€æŸ¥æ˜¯å¦å› è¾¾åˆ°é•¿åº¦é™åˆ¶è€Œæˆªæ–­
            if finish_reason == 'length':
                logger.warning(f"âš ï¸  å“åº”å› è¾¾åˆ°max_tokensé™åˆ¶è€Œè¢«æˆªæ–­")
                logger.warning(f"  - å½“å‰max_tokens: {max_tokens}")
                logger.warning(f"  - å»ºè®®: å¢åŠ max_tokenså‚æ•°ï¼ˆæ¨è2000+ï¼‰")
            
            if content:
                logger.info(f"  - è¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                logger.info(f"  - å®ŒæˆåŸå› : {finish_reason}")
                logger.info(f"  - è¿”å›å†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰: {content[:200]}")
                return content
            else:
                logger.error("âŒ AIè¿”å›äº†ç©ºå†…å®¹")
                logger.error(f"  - å®Œæ•´å“åº”: {data}")
                logger.error(f"  - å®ŒæˆåŸå› : {finish_reason}")
                
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                if finish_reason == 'length':
                    raise ValueError(f"AIå“åº”è¢«æˆªæ–­ä¸”æ— æœ‰æ•ˆå†…å®¹ã€‚è¯·å¢åŠ max_tokenså‚æ•°ï¼ˆå½“å‰: {max_tokens}ï¼Œå»ºè®®: 2000+ï¼‰")
                else:
                    raise ValueError(f"AIè¿”å›äº†ç©ºå†…å®¹ï¼ˆfinish_reason: {finish_reason}ï¼‰ï¼Œè¯·æ£€æŸ¥APIé…ç½®æˆ–ç¨åé‡è¯•")
            
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥ (HTTP {e.response.status_code})")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {e.response.text}")
            logger.error(f"  - æ¨¡å‹: {model}")
            raise Exception(f"APIè¿”å›é”™è¯¯ ({e.response.status_code}): {e.response.text}")
        except Exception as e:
            logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥")
            logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"  - æ¨¡å‹: {model}")
            raise
    

    async def _generate_openai_with_tools(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str],
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨OpenAIç”Ÿæˆæ–‡æœ¬ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰"""
        if not self.openai_http_client:
            raise ValueError("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨OpenAI APIï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - å·¥å…·æ•°é‡: {len(tools) if tools else 0}")
            
            url = f"{self.openai_base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openai_api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            # æ·»åŠ å·¥å…·å‚æ•°
            if tools:
                payload["tools"] = tools
                if tool_choice:
                    if tool_choice == "required":
                        payload["tool_choice"] = "required"
                    elif tool_choice == "auto":
                        payload["tool_choice"] = "auto"
                    elif tool_choice == "none":
                        payload["tool_choice"] = "none"
            
            response = await self.openai_http_client.post(url, headers=headers, json=payload)
            response.raise_for_status()
            
            data = response.json()
            
            logger.info(f"âœ… OpenAI APIè°ƒç”¨æˆåŠŸ")
            logger.debug(f"  - å®Œæ•´APIå“åº”: {data}")
            
            if not data.get('choices'):
                logger.error(f"âŒ APIè¿”å›çš„choicesä¸ºç©º")
                logger.error(f"  - å®Œæ•´å“åº”: {data}")
                logger.error(f"  - å“åº”é”®: {list(data.keys())}")
                raise ValueError(f"APIè¿”å›çš„å“åº”æ ¼å¼é”™è¯¯ï¼šchoiceså­—æ®µä¸ºç©ºã€‚å®Œæ•´å“åº”: {data}")
            
            choice = data['choices'][0]
            message = choice.get('message', {})
            finish_reason = choice.get('finish_reason')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_calls = message.get('tool_calls')
            if tool_calls:
                logger.info(f"ğŸ”§ AIè¯·æ±‚è°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·")
                return {
                    "tool_calls": tool_calls,
                    "content": message.get('content', ''),
                    "finish_reason": finish_reason
                }
            
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ™®é€šå†…å®¹
            content = message.get('content', '')
            if content:
                return {
                    "content": content,
                    "finish_reason": finish_reason
                }
            else:
                raise ValueError(f"AIè¿”å›äº†ç©ºå†…å®¹ï¼ˆfinish_reason: {finish_reason}ï¼‰")
            
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥ (HTTP {e.response.status_code})")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {e.response.text}")
            raise Exception(f"APIè¿”å›é”™è¯¯ ({e.response.status_code}): {e.response.text}")
        except Exception as e:
            logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _generate_anthropic_with_tools(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str],
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨Anthropicç”Ÿæˆæ–‡æœ¬ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰"""
        if not self.anthropic_client:
            raise ValueError("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨Anthropic APIï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - å·¥å…·æ•°é‡: {len(tools) if tools else 0}")
            
            kwargs = {
                "model": model,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "messages": [{"role": "user", "content": prompt}]
            }
            
            if system_prompt:
                kwargs["system"] = system_prompt
            
            # æ·»åŠ å·¥å…·å‚æ•°
            if tools:
                kwargs["tools"] = tools
                if tool_choice == "required":
                    kwargs["tool_choice"] = {"type": "any"}
                elif tool_choice == "auto":
                    kwargs["tool_choice"] = {"type": "auto"}
            
            response = await self.anthropic_client.messages.create(**kwargs)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_calls = []
            content_text = ""
            
            for block in response.content:
                if block.type == "tool_use":
                    tool_calls.append({
                        "id": block.id,
                        "type": "function",
                        "function": {
                            "name": block.name,
                            "arguments": block.input
                        }
                    })
                elif block.type == "text":
                    content_text += block.text
            
            if tool_calls:
                logger.info(f"ğŸ”§ AIè¯·æ±‚è°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·")
                return {
                    "tool_calls": tool_calls,
                    "content": content_text,
                    "finish_reason": response.stop_reason
                }
            
            return {
                "content": content_text,
                "finish_reason": response.stop_reason
            }
            
        except Exception as e:
            logger.error(f"âŒ Anthropic APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _generate_openai_stream(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨OpenAIæµå¼ç”Ÿæˆæ–‡æœ¬"""
        if not self.openai_http_client:
            raise ValueError("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨OpenAIæµå¼APIï¼ˆç›´æ¥HTTPè¯·æ±‚ï¼‰")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æœ€å¤§tokens: {max_tokens}")
            
            url = f"{self.openai_base_url}/chat/completions"
            headers = {
                "Authorization": f"Bearer {self.openai_api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True
            }
            
            async with self.openai_http_client.stream('POST', url, headers=headers, json=payload) as response:
                response.raise_for_status()
                logger.info(f"âœ… OpenAIæµå¼APIè¿æ¥æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")
                
                chunk_count = 0
                has_content = False
                finish_reason = None
                
                async for line in response.aiter_lines():
                    if line.startswith('data: '):
                        data_str = line[6:]
                        if data_str.strip() == '[DONE]':
                            break
                        
                        try:
                            import json
                            data = json.loads(data_str)
                            if 'choices' in data and len(data['choices']) > 0:
                                choice = data['choices'][0]
                                delta = choice.get('delta', {})
                                finish_reason = choice.get('finish_reason') or finish_reason
                                
                                # DeepSeek R1ç‰¹æ®Šå¤„ç†ï¼šåªæ”¶é›†contentï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œå¿½ç•¥reasoning_contentï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
                                # reasoning_contentæ˜¯AIçš„æ€è€ƒè¿‡ç¨‹ï¼Œä¸æ˜¯æˆ‘ä»¬éœ€è¦çš„JSONç»“æœ
                                content = delta.get('content', '')
                                
                                if content:
                                    chunk_count += 1
                                    has_content = True
                                    yield content
                        except json.JSONDecodeError:
                            continue
                
                # æ£€æŸ¥æ˜¯å¦å› é•¿åº¦é™åˆ¶æˆªæ–­
                if finish_reason == 'length':
                    logger.warning(f"âš ï¸  æµå¼å“åº”å› è¾¾åˆ°max_tokensé™åˆ¶è€Œè¢«æˆªæ–­")
                    logger.warning(f"  - å½“å‰max_tokens: {max_tokens}")
                    logger.warning(f"  - å»ºè®®: å¢åŠ max_tokenså‚æ•°ï¼ˆæ¨è2000+ï¼‰")
                
                if not has_content:
                    logger.warning(f"âš ï¸  æµå¼å“åº”æœªè¿”å›ä»»ä½•å†…å®¹")
                    logger.warning(f"  - å®ŒæˆåŸå› : {finish_reason}")
                
                logger.info(f"âœ… OpenAIæµå¼ç”Ÿæˆå®Œæˆï¼Œå…±æ¥æ”¶ {chunk_count} ä¸ªchunkï¼Œå®ŒæˆåŸå› : {finish_reason}")
            
        except httpx.TimeoutException as e:
            logger.error(f"âŒ OpenAIæµå¼APIè¶…æ—¶")
            logger.error(f"  - é”™è¯¯: {str(e)}")
            logger.error(f"  - æç¤º: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è€ƒè™‘ç¼©çŸ­prompté•¿åº¦")
            raise TimeoutError(f"AIæœåŠ¡è¶…æ—¶ï¼ˆ180ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•æˆ–å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦") from e
        except httpx.HTTPStatusError as e:
            logger.error(f"âŒ OpenAIæµå¼APIè°ƒç”¨å¤±è´¥ (HTTP {e.response.status_code})")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {await e.response.aread()}")
            raise
        except Exception as e:
            logger.error(f"âŒ OpenAIæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise
    
    async def _generate_anthropic(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> str:
        """ä½¿ç”¨Anthropicç”Ÿæˆæ–‡æœ¬"""
        if not self.anthropic_client:
            raise ValueError("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        try:
            response = await self.anthropic_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt or "",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        except Exception as e:
            logger.error(f"Anthropic APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    async def _generate_anthropic_stream(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨Anthropicæµå¼ç”Ÿæˆæ–‡æœ¬"""
        if not self.anthropic_client:
            raise ValueError("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨Anthropicæµå¼API")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æœ€å¤§tokens: {max_tokens}")
            
            async with self.anthropic_client.messages.stream(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt or "",
                messages=[{"role": "user", "content": prompt}]
            ) as stream:
                logger.info(f"âœ… Anthropicæµå¼APIè¿æ¥æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")
                
                chunk_count = 0
                async for text in stream.text_stream:
                    chunk_count += 1
                    yield text
                
                logger.info(f"âœ… Anthropicæµå¼ç”Ÿæˆå®Œæˆï¼Œå…±æ¥æ”¶ {chunk_count} ä¸ªchunk")
                
        except httpx.TimeoutException as e:
            logger.error(f"âŒ Anthropicæµå¼APIè¶…æ—¶")
            logger.error(f"  - é”™è¯¯: {str(e)}")
            raise TimeoutError(f"AIæœåŠ¡è¶…æ—¶ï¼ˆ180ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•æˆ–å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦") from e
        except Exception as e:
            logger.error(f"âŒ Anthropicæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise
    
    async def generate_text_with_mcp(
        self,
        prompt: str,
        user_id: str,
        db_session,
        enable_mcp: bool = True,
        max_tool_rounds: int = 3,
        tool_choice: str = "auto",
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ”¯æŒMCPå·¥å…·çš„AIæ–‡æœ¬ç”Ÿæˆï¼ˆéæµå¼ï¼‰
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            user_id: ç”¨æˆ·IDï¼Œç”¨äºè·å–MCPå·¥å…·
            db_session: æ•°æ®åº“ä¼šè¯
            enable_mcp: æ˜¯å¦å¯ç”¨MCPå¢å¼º
            max_tool_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ¬¡
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥ï¼ˆauto/required/noneï¼‰
            **kwargs: å…¶ä»–AIå‚æ•°ï¼ˆprovider, model, temperatureç­‰ï¼‰
        
        Returns:
            {
                "content": "AIç”Ÿæˆçš„æœ€ç»ˆæ–‡æœ¬",
                "tool_calls_made": 2,  # å®é™…è°ƒç”¨çš„å·¥å…·æ¬¡æ•°
                "tools_used": ["exa_search", "filesystem_read"],
                "finish_reason": "stop",
                "mcp_enhanced": True
            }
        """
        from app.services.mcp_tool_service import mcp_tool_service, MCPToolServiceError
        
        # åˆå§‹åŒ–è¿”å›ç»“æœ
        result = {
            "content": "",
            "tool_calls_made": 0,
            "tools_used": [],
            "finish_reason": "",
            "mcp_enhanced": False
        }
        
        # 1. è·å–MCPå·¥å…·ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        tools = None
        if enable_mcp:
            try:
                tools = await mcp_tool_service.get_user_enabled_tools(
                    user_id=user_id,
                    db_session=db_session
                )
                if tools:
                    logger.info(f"MCPå¢å¼º: åŠ è½½äº† {len(tools)} ä¸ªå·¥å…·")
                    result["mcp_enhanced"] = True
            except MCPToolServiceError as e:
                logger.error(f"è·å–MCPå·¥å…·å¤±è´¥ï¼Œé™çº§ä¸ºæ™®é€šç”Ÿæˆ: {e}")
                tools = None
        
        # 2. å·¥å…·è°ƒç”¨å¾ªç¯
        conversation_history = [
            {"role": "user", "content": prompt}
        ]
        
        for round_num in range(max_tool_rounds):
            logger.info(f"MCPå·¥å…·è°ƒç”¨è½®æ¬¡: {round_num + 1}/{max_tool_rounds}")
            
            # è°ƒç”¨AI
            ai_response = await self.generate_text(
                prompt=conversation_history[-1]["content"],
                tools=tools if round_num == 0 else None,  # åªåœ¨ç¬¬ä¸€è½®ä¼ é€’å·¥å…·
                tool_choice=tool_choice if round_num == 0 else None,
                **kwargs
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_calls = ai_response.get("tool_calls", [])
            
            if not tool_calls:
                # AIè¿”å›æœ€ç»ˆå†…å®¹
                result["content"] = ai_response.get("content", "")
                result["finish_reason"] = ai_response.get("finish_reason", "stop")
                break
            
            # 3. æ‰§è¡Œå·¥å…·è°ƒç”¨
            logger.info(f"AIè¯·æ±‚è°ƒç”¨ {len(tool_calls)} ä¸ªå·¥å…·")
            
            try:
                tool_results = await mcp_tool_service.execute_tool_calls(
                    user_id=user_id,
                    tool_calls=tool_calls,
                    db_session=db_session
                )
                
                # è®°å½•ä½¿ç”¨çš„å·¥å…·
                for tool_call in tool_calls:
                    tool_name = tool_call["function"]["name"]
                    if tool_name not in result["tools_used"]:
                        result["tools_used"].append(tool_name)
                
                result["tool_calls_made"] += len(tool_calls)
                
                # 4. æ„å»ºå·¥å…·ä¸Šä¸‹æ–‡
                tool_context = await mcp_tool_service.build_tool_context(
                    tool_results,
                    format="markdown"
                )
                
                # 5. æ›´æ–°å¯¹è¯å†å²
                conversation_history.append({
                    "role": "assistant",
                    "content": ai_response.get("content", ""),
                    "tool_calls": tool_calls
                })
                
                for tool_result in tool_results:
                    conversation_history.append({
                        "role": "tool",
                        "tool_call_id": tool_result["tool_call_id"],
                        "content": tool_result["content"]
                    })
                
                # 6. æ„å»ºä¸‹ä¸€è½®æç¤º
                next_prompt = (
                    f"{prompt}\n\n"
                    f"{tool_context}\n\n"
                    f"è¯·åŸºäºä»¥ä¸Šå·¥å…·æŸ¥è¯¢ç»“æœï¼Œç»§ç»­å®Œæˆä»»åŠ¡ã€‚"
                )
                conversation_history.append({
                    "role": "user",
                    "content": next_prompt
                })
                
            except Exception as e:
                logger.error(f"æ‰§è¡ŒMCPå·¥å…·å¤±è´¥: {e}", exc_info=True)
                # é™çº§ï¼šè¿”å›å½“å‰AIå“åº”
                result["content"] = ai_response.get("content", "")
                result["finish_reason"] = "tool_error"
                break
        
        else:
            # è¾¾åˆ°æœ€å¤§è½®æ¬¡
            logger.warning(f"è¾¾åˆ°MCPæœ€å¤§è°ƒç”¨è½®æ¬¡ {max_tool_rounds}")
            result["content"] = conversation_history[-1].get("content", "")
            result["finish_reason"] = "max_rounds"
        
        return result
    
    async def generate_text_stream_with_mcp(
        self,
        prompt: str,
        user_id: str,
        db_session,
        enable_mcp: bool = True,
        mcp_planning_prompt: Optional[str] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        æ”¯æŒMCPå·¥å…·çš„AIæµå¼æ–‡æœ¬ç”Ÿæˆï¼ˆä¸¤é˜¶æ®µæ¨¡å¼ï¼‰
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            user_id: ç”¨æˆ·ID
            db_session: æ•°æ®åº“ä¼šè¯
            enable_mcp: æ˜¯å¦å¯ç”¨MCPå¢å¼º
            mcp_planning_prompt: MCPè§„åˆ’é˜¶æ®µçš„æç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–AIå‚æ•°
        
        Yields:
            æµå¼æ–‡æœ¬chunk
        """
        from app.services.mcp_tool_service import mcp_tool_service
        
        # é˜¶æ®µ1: å·¥å…·è°ƒç”¨é˜¶æ®µï¼ˆéæµå¼ï¼‰
        enhanced_prompt = prompt
        
        if enable_mcp:
            try:
                # è·å–MCPå·¥å…·
                tools = await mcp_tool_service.get_user_enabled_tools(
                    user_id=user_id,
                    db_session=db_session
                )
                
                if tools:
                    logger.info(f"MCPå¢å¼ºï¼ˆæµå¼ï¼‰: åŠ è½½äº† {len(tools)} ä¸ªå·¥å…·")
                    
                    # ä½¿ç”¨è§„åˆ’æç¤ºè®©AIå†³å®šéœ€è¦æŸ¥è¯¢ä»€ä¹ˆ
                    if not mcp_planning_prompt:
                        mcp_planning_prompt = (
                            f"ä»»åŠ¡: {prompt}\n\n"
                            f"è¯·åˆ†æè¿™ä¸ªä»»åŠ¡ï¼Œå†³å®šæ˜¯å¦éœ€è¦æŸ¥è¯¢å¤–éƒ¨ä¿¡æ¯ã€‚"
                            f"å¦‚æœéœ€è¦ï¼Œè¯·è°ƒç”¨ç›¸åº”çš„å·¥å…·è·å–ä¿¡æ¯ã€‚"
                        )
                    
                    # éæµå¼è°ƒç”¨è·å–å·¥å…·ç»“æœ
                    planning_result = await self.generate_text_with_mcp(
                        prompt=mcp_planning_prompt,
                        user_id=user_id,
                        db_session=db_session,
                        enable_mcp=True,
                        max_tool_rounds=2,
                        tool_choice="auto",
                        **kwargs
                    )
                    
                    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œå°†ç»“æœèå…¥æç¤º
                    if planning_result["tool_calls_made"] > 0:
                        enhanced_prompt = (
                            f"{prompt}\n\n"
                            f"ã€å‚è€ƒèµ„æ–™ã€‘\n"
                            f"{planning_result.get('content', '')}"
                        )
                        logger.info(
                            f"MCPå·¥å…·è§„åˆ’å®Œæˆï¼Œè°ƒç”¨äº† "
                            f"{planning_result['tool_calls_made']} æ¬¡å·¥å…·"
                        )
            
            except Exception as e:
                logger.error(f"MCPå·¥å…·è§„åˆ’å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æç¤º: {e}")
        
        # é˜¶æ®µ2: å†…å®¹ç”Ÿæˆé˜¶æ®µï¼ˆæµå¼ï¼‰
        async for chunk in self.generate_text_stream(
            prompt=enhanced_prompt,
            **kwargs
        ):
            yield chunk


# åˆ›å»ºå…¨å±€AIæœåŠ¡å®ä¾‹
ai_service = AIService()


def create_user_ai_service(
    api_provider: str,
    api_key: str,
    api_base_url: str,
    model_name: str,
    temperature: float,
    max_tokens: int
) -> AIService:
    """
    æ ¹æ®ç”¨æˆ·è®¾ç½®åˆ›å»ºAIæœåŠ¡å®ä¾‹
    
    Args:
        api_provider: APIæä¾›å•†
        api_key: APIå¯†é’¥
        api_base_url: APIåŸºç¡€URL
        model_name: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokens
        
    Returns:
        AIServiceå®ä¾‹
    """
    return AIService(
        api_provider=api_provider,
        api_key=api_key,
        api_base_url=api_base_url,
        default_model=model_name,
        default_temperature=temperature,
        default_max_tokens=max_tokens
    )